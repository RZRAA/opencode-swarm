# OpenCode Swarm v6.3 — Pre-Reviewer Tool Pipeline

**Version**: 6.3.0 (Minor)
**Scope**: `imports`, `lint`, `secretscan` tools; unified pre-reviewer pipeline in architect workflow
**Breaking Changes**: Three new tools registered; architect Phase 5 workflow rewritten to include full pre-reviewer sequence
**Dependencies**: v6.0 (`diff` tool), v6.1 (designer gate in Phase 5), v6.2 (retrospective in Phase 6)
**Prerequisite**: The `diff` tool from v6.0 must already be implemented. This release completes the pre-reviewer pipeline that `diff` started.

---

## Overview

v6.0 introduced the `diff` tool for contract change detection and delegated integration analysis to the `explorer` agent via grep-based consumer search. This release replaces that fragile grep approach with a deterministic AST-based `imports` tool, and adds `lint` and `secretscan` as pre-reviewer gates that catch structural issues and credential leaks before the reviewer spends attention on them.

After v6.3, the architect's pre-reviewer sequence is:

```
coder → diff + imports (integration check) → lint fix → lint check → secretscan → reviewer
```

---

## 1. `imports` — AST-Based Import Graph

### Purpose

Given a file path or exported symbol name, returns every file that imports from it, which specific exports each consumer uses, and the line numbers. Replaces the explorer's grep-based consumer analysis with deterministic graph traversal.

### Implementation

#### `src/tools/imports.ts` (new file)

```typescript
import { execSync } from 'node:child_process';
import * as fs from 'node:fs';
import * as path from 'node:path';
import { type ToolDefinition, tool } from '@opencode-ai/plugin/tool';

/**
 * Ecosystem detection — determines which import resolution strategy to use.
 */
function detectEcosystem(cwd: string): 'typescript' | 'python' | 'rust' | 'unknown' {
  if (fs.existsSync(path.join(cwd, 'tsconfig.json')) ||
      fs.existsSync(path.join(cwd, 'package.json'))) {
    return 'typescript';
  }
  if (fs.existsSync(path.join(cwd, 'pyproject.toml')) ||
      fs.existsSync(path.join(cwd, 'setup.py')) ||
      fs.existsSync(path.join(cwd, 'requirements.txt'))) {
    return 'python';
  }
  if (fs.existsSync(path.join(cwd, 'Cargo.toml'))) {
    return 'rust';
  }
  return 'unknown';
}

/**
 * TypeScript/JavaScript import analysis using grep + regex AST approximation.
 *
 * Full TS compiler API would be ideal but adds a heavy dependency.
 * This approach handles: import { X } from './path', import X from './path',
 * require('./path'), dynamic import('./path'), export { X } from './path'.
 *
 * For v1, this is a high-accuracy regex scanner. v2 can upgrade to
 * tree-sitter or TS compiler API if edge cases demand it.
 */
function findTSConsumers(
  targetFile: string,
  cwd: string,
  symbol?: string,
): string {
  const targetBase = targetFile
    .replace(/\.(ts|tsx|js|jsx|mjs|cjs)$/, '')
    .replace(/\/index$/, '');

  // Build patterns that match imports of the target file
  // Handle: relative paths, aliases, barrel exports
  const targetName = path.basename(targetBase);
  const targetRelDir = path.dirname(targetFile);

  // grep for any import/require referencing the target module
  // Use -r for recursive, -n for line numbers, -l initially to find files
  const extensions = '*.ts,*.tsx,*.js,*.jsx,*.mjs,*.cjs';
  const includeFlags = extensions.split(',').map(e => `--include="${e}"`).join(' ');

  try {
    // Phase 1: Find all files that reference the target module name
    const grepPattern = `(from|require)\\s*[('"\`].*${escapeRegex(targetName)}`;
    const rawMatches = execSync(
      `grep -rn ${includeFlags} -E '${grepPattern}' . 2>/dev/null || true`,
      { cwd, encoding: 'utf-8', timeout: 15000, maxBuffer: 2 * 1024 * 1024 }
    ).trim();

    if (!rawMatches) {
      return JSON.stringify({
        target: targetFile,
        symbol: symbol || null,
        consumers: [],
        count: 0,
        message: 'No consumers found',
      }, null, 2);
    }

    // Phase 2: Parse matches into structured results
    const consumers: Array<{
      file: string;
      line: number;
      imports: string[];
      importType: 'named' | 'default' | 'namespace' | 'require' | 'reexport' | 'dynamic';
      raw: string;
    }> = [];

    for (const match of rawMatches.split('\n').filter(Boolean)) {
      const lineMatch = match.match(/^\.\/(.+?):(\d+):(.+)$/);
      if (!lineMatch) continue;

      const [, file, lineStr, content] = lineMatch;
      const line = parseInt(lineStr, 10);

      // Skip self-references
      if (file === targetFile) continue;

      // Verify this actually imports from our target (not a similarly-named module)
      if (!isImportOfTarget(content, targetFile, targetBase, cwd, file)) continue;

      // Extract imported symbols
      const importInfo = parseImportLine(content);

      // If symbol filter is specified, only include consumers that import that symbol
      if (symbol && importInfo.imports.length > 0 &&
          !importInfo.imports.includes(symbol) &&
          importInfo.importType !== 'namespace' &&
          importInfo.importType !== 'default') {
        continue;
      }

      consumers.push({
        file,
        line,
        imports: importInfo.imports,
        importType: importInfo.importType,
        raw: content.trim(),
      });
    }

    return JSON.stringify({
      target: targetFile,
      symbol: symbol || null,
      consumers,
      count: consumers.length,
    }, null, 2);
  } catch (e) {
    return JSON.stringify({
      target: targetFile,
      error: (e as Error).message,
      consumers: [],
      count: 0,
    }, null, 2);
  }
}

/**
 * Python import analysis using grep.
 */
function findPythonConsumers(
  targetFile: string,
  cwd: string,
  symbol?: string,
): string {
  const targetModule = targetFile
    .replace(/\.py$/, '')
    .replace(/\//g, '.')
    .replace(/__init__$/, '')
    .replace(/\.$/, '');

  const moduleParts = targetModule.split('.');
  const moduleName = moduleParts[moduleParts.length - 1];

  try {
    const grepPattern = `(from|import)\\s+.*${escapeRegex(moduleName)}`;
    const rawMatches = execSync(
      `grep -rn --include="*.py" -E '${grepPattern}' . 2>/dev/null || true`,
      { cwd, encoding: 'utf-8', timeout: 15000, maxBuffer: 2 * 1024 * 1024 }
    ).trim();

    if (!rawMatches) {
      return JSON.stringify({
        target: targetFile,
        symbol: symbol || null,
        consumers: [],
        count: 0,
        message: 'No consumers found',
      }, null, 2);
    }

    const consumers: Array<{
      file: string;
      line: number;
      imports: string[];
      raw: string;
    }> = [];

    for (const match of rawMatches.split('\n').filter(Boolean)) {
      const lineMatch = match.match(/^\.\/(.+?):(\d+):(.+)$/);
      if (!lineMatch) continue;
      const [, file, lineStr, content] = lineMatch;
      if (file === targetFile) continue;

      const imports: string[] = [];
      const fromMatch = content.match(/from\s+\S+\s+import\s+(.+)/);
      if (fromMatch) {
        imports.push(...fromMatch[1].split(',').map(s => s.trim().split(/\s+as\s+/)[0]));
      }

      if (symbol && imports.length > 0 && !imports.includes(symbol)) continue;

      consumers.push({
        file,
        line: parseInt(lineStr, 10),
        imports,
        raw: content.trim(),
      });
    }

    return JSON.stringify({
      target: targetFile,
      symbol: symbol || null,
      consumers,
      count: consumers.length,
    }, null, 2);
  } catch (e) {
    return JSON.stringify({
      target: targetFile,
      error: (e as Error).message,
      consumers: [],
      count: 0,
    }, null, 2);
  }
}

/**
 * Verify that an import line actually references our target file
 * by resolving the relative path from the consuming file.
 */
function isImportOfTarget(
  content: string,
  targetFile: string,
  targetBase: string,
  cwd: string,
  consumerFile: string,
): boolean {
  // Extract the module specifier from the import statement
  const specMatch = content.match(/(?:from|require)\s*\(?\s*['"`]([^'"`]+)['"`]/);
  if (!specMatch) return false;

  const specifier = specMatch[1];

  // Skip node_modules / bare specifiers (no relative path)
  if (!specifier.startsWith('.') && !specifier.startsWith('/')) return false;

  // Resolve the specifier relative to the consumer file
  const consumerDir = path.dirname(consumerFile);
  const resolved = path.normalize(path.join(consumerDir, specifier))
    .replace(/\.(ts|tsx|js|jsx|mjs|cjs)$/, '')
    .replace(/\/index$/, '');

  const normalizedTarget = targetBase.replace(/^\.\//, '');
  const normalizedResolved = resolved.replace(/^\.\//, '');

  return normalizedResolved === normalizedTarget ||
         normalizedResolved === normalizedTarget.replace(/\/index$/, '');
}

/**
 * Parse an import line to extract symbol names and import type.
 */
function parseImportLine(content: string): {
  imports: string[];
  importType: 'named' | 'default' | 'namespace' | 'require' | 'reexport' | 'dynamic';
} {
  // Named: import { X, Y } from '...'
  const namedMatch = content.match(/import\s*\{([^}]+)\}\s*from/);
  if (namedMatch) {
    const imports = namedMatch[1].split(',').map(s => {
      const parts = s.trim().split(/\s+as\s+/);
      return parts[0].trim();
    }).filter(Boolean);
    return { imports, importType: 'named' };
  }

  // Namespace: import * as X from '...'
  if (/import\s+\*\s+as\s+\w+/.test(content)) {
    return { imports: ['*'], importType: 'namespace' };
  }

  // Default: import X from '...'
  const defaultMatch = content.match(/import\s+(\w+)\s+from/);
  if (defaultMatch) {
    return { imports: [defaultMatch[1]], importType: 'default' };
  }

  // Re-export: export { X } from '...'
  const reexportMatch = content.match(/export\s*\{([^}]+)\}\s*from/);
  if (reexportMatch) {
    const imports = reexportMatch[1].split(',').map(s => s.trim().split(/\s+as\s+/)[0]).filter(Boolean);
    return { imports, importType: 'reexport' };
  }

  // Require: const X = require('...')
  const requireMatch = content.match(/(?:const|let|var)\s+(?:\{([^}]+)\}|(\w+))\s*=\s*require/);
  if (requireMatch) {
    if (requireMatch[1]) {
      return { imports: requireMatch[1].split(',').map(s => s.trim()).filter(Boolean), importType: 'require' };
    }
    return { imports: [requireMatch[2]], importType: 'require' };
  }

  // Dynamic: import('...')
  if (/import\s*\(/.test(content)) {
    return { imports: [], importType: 'dynamic' };
  }

  return { imports: [], importType: 'named' };
}

function escapeRegex(str: string): string {
  return str.replace(/[.*+?^${}()|[\]\\]/g, '\\$&');
}

/**
 * Import graph tool — find all consumers of a file or symbol.
 */
export const imports: ToolDefinition = tool({
  description:
    'Find all files that import from a given source file. Returns consumers with ' +
    'specific imported symbols and line numbers. Supports TypeScript/JavaScript and Python. ' +
    'Use after `diff` detects contract changes to find affected consumers.',
  args: {
    file: tool.schema
      .string()
      .describe('Source file path to find consumers of (e.g., "src/auth/login.ts")'),
    symbol: tool.schema
      .string()
      .optional()
      .describe('Optional: specific exported symbol to trace (e.g., "login"). If omitted, finds all consumers of the file.'),
  },
  execute: async (args, ctx?: { abortSignal?: AbortSignal }) => {
    const cwd = process.cwd();
    const ecosystem = detectEcosystem(cwd);

    switch (ecosystem) {
      case 'typescript':
        return findTSConsumers(args.file, cwd, args.symbol);
      case 'python':
        return findPythonConsumers(args.file, cwd, args.symbol);
      default:
        // Fallback: basic grep for any ecosystem
        return findTSConsumers(args.file, cwd, args.symbol);
    }
  },
});
```

### Architect Integration

The `imports` tool replaces the explorer grep-based consumer analysis in the integration analysis workflow. The architect's Rule 8 (from v6.0) is updated:

**Replace Rule 8** in `src/agents/architect.ts`:

```
8. **INTEGRATION ANALYSIS**: After {{AGENT_PREFIX}}coder completes a task, run the `diff` tool. If `hasContractChanges` is true, run the `imports` tool for EACH file with contract changes to get the complete consumer list. If any consumers would break (calling with old signatures, missing newly required imports), send the full list of affected files and line numbers back to {{AGENT_PREFIX}}coder. Do NOT delegate to {{AGENT_PREFIX}}explorer for integration analysis — the `imports` tool provides deterministic results. Only delegate to {{AGENT_PREFIX}}explorer if `imports` returns an error or the ecosystem is unsupported.
```

**Updated delegation example** (replaces the explorer integration analysis example from v6.0):

```
# Integration analysis is now tool-based, not agent-based:
# 1. Run diff tool → check hasContractChanges
# 2. If yes, for each changed file with contract changes:
#    Run imports tool with file path
# 3. Review consumer list for breaking changes
# 4. If breaking → back to coder with affected file list

# Example tool calls:
# diff(base: "HEAD")
#   → { hasContractChanges: true, contractChanges: ["+export function login(email, opts)..."] }
# imports(file: "src/auth/login.ts", symbol: "login")
#   → { consumers: [{ file: "src/api/routes.ts", line: 42, imports: ["login"], importType: "named" }] }
```

---

## 2. `lint` — Project Linter with Fix Mode

### Purpose

Runs the project's configured linter and returns structured diagnostics. Supports two modes: `fix` (auto-apply fixable issues, report remaining) and `check` (report only). The architect runs `fix` first, then `check` — only non-auto-fixable issues go back to coder.

### Implementation

#### `src/tools/lint.ts` (new file)

```typescript
import { execSync } from 'node:child_process';
import * as fs from 'node:fs';
import * as path from 'node:path';
import { type ToolDefinition, tool } from '@opencode-ai/plugin/tool';

interface LintResult {
  linter: string;
  mode: 'fix' | 'check';
  fixesApplied: number;
  diagnostics: Array<{
    file: string;
    line: number;
    column: number;
    severity: 'error' | 'warning' | 'info';
    rule: string;
    message: string;
    fixable: boolean;
  }>;
  errorCount: number;
  warningCount: number;
  success: boolean;
}

/**
 * Detect which linter is configured for the project.
 * Returns the linter name and the base command.
 */
function detectLinter(cwd: string): { name: string; cmd: string; fixCmd: string } | null {
  // Check biome.json / biome.jsonc
  if (fs.existsSync(path.join(cwd, 'biome.json')) ||
      fs.existsSync(path.join(cwd, 'biome.jsonc'))) {
    return {
      name: 'biome',
      cmd: 'npx @biomejs/biome check --reporter=json',
      fixCmd: 'npx @biomejs/biome check --fix --reporter=json',
    };
  }

  // Check .eslintrc* or eslint.config.*
  const eslintConfigs = [
    '.eslintrc', '.eslintrc.js', '.eslintrc.cjs', '.eslintrc.json', '.eslintrc.yaml', '.eslintrc.yml',
    'eslint.config.js', 'eslint.config.mjs', 'eslint.config.cjs', 'eslint.config.ts',
  ];
  if (eslintConfigs.some(c => fs.existsSync(path.join(cwd, c)))) {
    return {
      name: 'eslint',
      cmd: 'npx eslint --format=json .',
      fixCmd: 'npx eslint --fix --format=json .',
    };
  }

  // Check pyproject.toml for ruff
  const pyproject = path.join(cwd, 'pyproject.toml');
  if (fs.existsSync(pyproject)) {
    const content = fs.readFileSync(pyproject, 'utf-8');
    if (content.includes('[tool.ruff]') || content.includes('ruff')) {
      return {
        name: 'ruff',
        cmd: 'ruff check --output-format=json .',
        fixCmd: 'ruff check --fix --output-format=json .',
      };
    }
  }

  // Check Cargo.toml for clippy
  if (fs.existsSync(path.join(cwd, 'Cargo.toml'))) {
    return {
      name: 'clippy',
      cmd: 'cargo clippy --message-format=json 2>&1',
      fixCmd: 'cargo clippy --fix --allow-dirty --message-format=json 2>&1',
    };
  }

  // Check for PSScriptAnalyzer (PowerShell)
  const ps1Files = execSync('find . -name "*.ps1" -maxdepth 3 2>/dev/null | head -1', {
    cwd, encoding: 'utf-8', timeout: 5000,
  }).trim();
  if (ps1Files) {
    return {
      name: 'psscriptanalyzer',
      cmd: 'pwsh -Command "Invoke-ScriptAnalyzer -Path . -Recurse -Severity Error,Warning | ConvertTo-Json"',
      fixCmd: 'pwsh -Command "Invoke-ScriptAnalyzer -Path . -Recurse -Severity Error,Warning -Fix | ConvertTo-Json"',
    };
  }

  // Fallback: check package.json for lint script
  const pkgPath = path.join(cwd, 'package.json');
  if (fs.existsSync(pkgPath)) {
    try {
      const pkg = JSON.parse(fs.readFileSync(pkgPath, 'utf-8'));
      if (pkg.scripts?.lint) {
        return {
          name: 'package-lint',
          cmd: 'npm run lint -- --format=json 2>/dev/null || npm run lint',
          fixCmd: 'npm run lint -- --fix --format=json 2>/dev/null || npm run lint -- --fix',
        };
      }
    } catch { /* ignore parse errors */ }
  }

  return null;
}

/**
 * Parse ESLint JSON output into normalized diagnostics.
 */
function parseESLintOutput(output: string): LintResult['diagnostics'] {
  try {
    const results = JSON.parse(output) as Array<{
      filePath: string;
      messages: Array<{
        line: number;
        column: number;
        severity: 1 | 2;
        ruleId: string | null;
        message: string;
        fix?: unknown;
      }>;
    }>;

    return results.flatMap(file =>
      file.messages.map(msg => ({
        file: file.filePath,
        line: msg.line,
        column: msg.column,
        severity: msg.severity === 2 ? 'error' as const : 'warning' as const,
        rule: msg.ruleId || 'unknown',
        message: msg.message,
        fixable: !!msg.fix,
      }))
    );
  } catch {
    return [];
  }
}

/**
 * Parse Biome JSON output into normalized diagnostics.
 */
function parseBiomeOutput(output: string): LintResult['diagnostics'] {
  try {
    const result = JSON.parse(output);
    const diagnostics = result.diagnostics || [];
    return diagnostics.map((d: {
      location?: { path?: { file?: string }; span?: { start?: number } };
      severity?: string;
      category?: string;
      message?: Array<{ content?: string }>;
    }) => ({
      file: d.location?.path?.file || 'unknown',
      line: d.location?.span?.start || 0,
      column: 0,
      severity: d.severity === 'error' ? 'error' as const : 'warning' as const,
      rule: d.category || 'unknown',
      message: d.message?.map((m: { content?: string }) => m.content).join('') || '',
      fixable: false,
    }));
  } catch {
    return [];
  }
}

/**
 * Parse Ruff JSON output into normalized diagnostics.
 */
function parseRuffOutput(output: string): LintResult['diagnostics'] {
  try {
    const results = JSON.parse(output) as Array<{
      filename: string;
      location: { row: number; column: number };
      code: string;
      message: string;
      fix?: { applicability?: string };
    }>;

    return results.map(r => ({
      file: r.filename,
      line: r.location.row,
      column: r.location.column,
      severity: 'error' as const,
      rule: r.code,
      message: r.message,
      fixable: r.fix?.applicability === 'safe',
    }));
  } catch {
    return [];
  }
}

/**
 * Generic fallback parser for unstructured linter output.
 * Attempts line:col:message pattern extraction.
 */
function parseGenericOutput(output: string): LintResult['diagnostics'] {
  const diagnostics: LintResult['diagnostics'] = [];
  const linePattern = /^(.+?):(\d+):(\d+):\s*(error|warning|info|Error|Warning)[\s:]+(.+)$/gm;
  let match: RegExpExecArray | null;

  while ((match = linePattern.exec(output)) !== null) {
    diagnostics.push({
      file: match[1],
      line: parseInt(match[2], 10),
      column: parseInt(match[3], 10),
      severity: match[4].toLowerCase() as 'error' | 'warning' | 'info',
      rule: 'unknown',
      message: match[5].trim(),
      fixable: false,
    });
  }

  return diagnostics;
}

export const lint: ToolDefinition = tool({
  description:
    'Run the project\'s configured linter (Biome, ESLint, Ruff, clippy, PSScriptAnalyzer). ' +
    'Two modes: "fix" auto-applies fixable issues and reports remaining; "check" reports only. ' +
    'Returns structured diagnostics with file, line, rule, severity. ' +
    'Use after coder completes, before reviewer — fixes go back to coder, not reviewer.',
  args: {
    mode: tool.schema
      .enum(['fix', 'check'])
      .default('check')
      .describe('"fix" auto-applies fixable issues then reports remaining. "check" reports only.'),
    paths: tool.schema
      .string()
      .optional()
      .describe('Optional: specific file or directory to lint (default: entire project)'),
  },
  execute: async (args) => {
    const cwd = process.cwd();
    const linter = detectLinter(cwd);

    if (!linter) {
      return JSON.stringify({
        linter: 'none',
        mode: args.mode,
        fixesApplied: 0,
        diagnostics: [],
        errorCount: 0,
        warningCount: 0,
        success: true,
        message: 'No linter detected. Checked for: biome, eslint, ruff, clippy, psscriptanalyzer, package.json lint script.',
      }, null, 2);
    }

    const cmd = args.mode === 'fix' ? linter.fixCmd : linter.cmd;
    const pathSuffix = args.paths ? ` ${args.paths}` : '';

    try {
      let output: string;
      try {
        output = execSync(`${cmd}${pathSuffix}`, {
          cwd,
          encoding: 'utf-8',
          timeout: 60000,
          maxBuffer: 5 * 1024 * 1024,
          // Linters often exit non-zero when they find issues
          stdio: ['pipe', 'pipe', 'pipe'],
        });
      } catch (e: unknown) {
        // Linters exit non-zero on findings — capture stdout anyway
        const execError = e as { stdout?: string; stderr?: string; status?: number };
        output = execError.stdout || execError.stderr || '';
      }

      // Parse based on linter type
      let diagnostics: LintResult['diagnostics'];
      switch (linter.name) {
        case 'eslint':
          diagnostics = parseESLintOutput(output);
          break;
        case 'biome':
          diagnostics = parseBiomeOutput(output);
          break;
        case 'ruff':
          diagnostics = parseRuffOutput(output);
          break;
        default:
          diagnostics = parseGenericOutput(output);
      }

      const errorCount = diagnostics.filter(d => d.severity === 'error').length;
      const warningCount = diagnostics.filter(d => d.severity === 'warning').length;
      const fixableCount = diagnostics.filter(d => d.fixable).length;

      const result: LintResult = {
        linter: linter.name,
        mode: args.mode as 'fix' | 'check',
        fixesApplied: args.mode === 'fix' ? fixableCount : 0,
        diagnostics,
        errorCount,
        warningCount,
        success: errorCount === 0,
      };

      return JSON.stringify(result, null, 2);
    } catch (e) {
      return JSON.stringify({
        linter: linter.name,
        mode: args.mode,
        error: (e as Error).message,
        diagnostics: [],
        errorCount: 0,
        warningCount: 0,
        success: false,
      }, null, 2);
    }
  },
});
```

---

## 3. `secretscan` — Entropy-Based Credential Detection

### Purpose

Scans files or diffs for high-entropy strings that indicate leaked secrets (API keys, tokens, JWTs, private keys). Combines Shannon entropy analysis with known-pattern regex matching. Distinct from `lint` which only catches rule-based patterns — this catches `sk-proj-aB3x...yz98` without needing a keyword next to it.

### Implementation

#### `src/tools/secretscan.ts` (new file)

```typescript
import { execSync } from 'node:child_process';
import * as fs from 'node:fs';
import * as path from 'node:path';
import { type ToolDefinition, tool } from '@opencode-ai/plugin/tool';

/**
 * Known secret patterns — regex + human-readable type name.
 * These catch secrets that have recognizable prefixes or formats.
 */
const SECRET_PATTERNS: Array<{ name: string; pattern: RegExp; severity: 'critical' | 'high' | 'medium' }> = [
  // API Keys with known prefixes
  { name: 'AWS Access Key', pattern: /AKIA[0-9A-Z]{16}/, severity: 'critical' },
  { name: 'AWS Secret Key', pattern: /(?:aws_secret_access_key|secret_key)\s*[:=]\s*['"]?[A-Za-z0-9/+=]{40}/, severity: 'critical' },
  { name: 'GitHub Token', pattern: /gh[ps]_[A-Za-z0-9_]{36,}/, severity: 'critical' },
  { name: 'GitHub Fine-grained Token', pattern: /github_pat_[A-Za-z0-9_]{22,}/, severity: 'critical' },
  { name: 'Slack Token', pattern: /xox[bpors]-[A-Za-z0-9-]+/, severity: 'critical' },
  { name: 'OpenAI API Key', pattern: /sk-[A-Za-z0-9]{20,}/, severity: 'critical' },
  { name: 'Anthropic API Key', pattern: /sk-ant-[A-Za-z0-9_-]{40,}/, severity: 'critical' },
  { name: 'Stripe Key', pattern: /[sr]k_(live|test)_[A-Za-z0-9]{20,}/, severity: 'critical' },
  { name: 'Google API Key', pattern: /AIza[A-Za-z0-9_-]{35}/, severity: 'critical' },
  { name: 'Twilio Key', pattern: /SK[0-9a-fA-F]{32}/, severity: 'high' },
  { name: 'SendGrid Key', pattern: /SG\.[A-Za-z0-9_-]{22}\.[A-Za-z0-9_-]{43}/, severity: 'critical' },
  { name: 'npm Token', pattern: /npm_[A-Za-z0-9]{36}/, severity: 'critical' },

  // Private keys
  { name: 'RSA Private Key', pattern: /-----BEGIN (?:RSA )?PRIVATE KEY-----/, severity: 'critical' },
  { name: 'EC Private Key', pattern: /-----BEGIN EC PRIVATE KEY-----/, severity: 'critical' },
  { name: 'SSH Private Key', pattern: /-----BEGIN OPENSSH PRIVATE KEY-----/, severity: 'critical' },

  // JWT (usually not itself a secret, but hardcoded JWTs suggest leaked auth)
  { name: 'JWT Token', pattern: /eyJ[A-Za-z0-9_-]{10,}\.eyJ[A-Za-z0-9_-]{10,}\.[A-Za-z0-9_-]{10,}/, severity: 'high' },

  // Generic patterns
  { name: 'Hardcoded Password', pattern: /(?:password|passwd|pwd)\s*[:=]\s*['"][^'"]{8,}['"]/, severity: 'high' },
  { name: 'Hardcoded Secret', pattern: /(?:secret|api_key|apikey|api_secret|access_token)\s*[:=]\s*['"][^'"]{8,}['"]/, severity: 'high' },
  { name: 'Connection String', pattern: /(?:mongodb|postgres|mysql|redis):\/\/[^\s'"]+:[^\s'"]+@/, severity: 'critical' },
  { name: 'Bearer Token', pattern: /(?:Bearer|Authorization)\s*[:=]\s*['"][A-Za-z0-9._-]{20,}['"]/, severity: 'high' },
];

/**
 * Files/paths to always skip during scanning.
 */
const SKIP_PATTERNS = [
  /node_modules/,
  /\.git\//,
  /dist\//,
  /build\//,
  /\.min\.(js|css)$/,
  /package-lock\.json$/,
  /yarn\.lock$/,
  /bun\.lockb$/,
  /\.map$/,
  /\.test\.(ts|js|tsx|jsx)$/,   // Test files may have mock secrets
  /\.spec\.(ts|js|tsx|jsx)$/,
  /fixtures?\//,
  /mocks?\//,
  /\.snap$/,
];

/**
 * Calculate Shannon entropy of a string.
 * High entropy (>4.5 for 20+ char strings) suggests random/generated content.
 */
function shannonEntropy(str: string): number {
  if (str.length === 0) return 0;
  const freq = new Map<string, number>();
  for (const char of str) {
    freq.set(char, (freq.get(char) || 0) + 1);
  }
  let entropy = 0;
  for (const count of freq.values()) {
    const p = count / str.length;
    entropy -= p * Math.log2(p);
  }
  return entropy;
}

/**
 * Extract high-entropy strings from a line.
 * Looks for quoted strings, assignment values, and bare tokens.
 */
function extractHighEntropyStrings(line: string): Array<{ value: string; entropy: number }> {
  const results: Array<{ value: string; entropy: number }> = [];

  // Extract quoted strings (single, double, backtick)
  const quotedPattern = /['"`]([A-Za-z0-9+/=_-]{20,})['"`]/g;
  let match: RegExpExecArray | null;

  while ((match = quotedPattern.exec(line)) !== null) {
    const value = match[1];
    const entropy = shannonEntropy(value);
    // Threshold: entropy > 4.5 for 20+ char strings suggests randomness
    if (entropy > 4.5 && value.length >= 20) {
      results.push({ value: value.substring(0, 40) + (value.length > 40 ? '...' : ''), entropy });
    }
  }

  return results;
}

interface SecretFinding {
  file: string;
  line: number;
  type: string;
  severity: 'critical' | 'high' | 'medium';
  detail: string;
  detectionMethod: 'pattern' | 'entropy';
}

export const secretscan: ToolDefinition = tool({
  description:
    'Scan files for leaked secrets: API keys, tokens, private keys, hardcoded passwords. ' +
    'Combines known-pattern regex matching with Shannon entropy analysis for unknown secret formats. ' +
    'Runs after lint, before reviewer. Catches credentials that no prompt-driven agent reliably detects.',
  args: {
    scope: tool.schema
      .enum(['diff', 'files', 'all'])
      .default('diff')
      .describe('"diff" scans only changed lines (fastest). "files" scans specific paths. "all" scans entire project.'),
    paths: tool.schema
      .string()
      .optional()
      .describe('Comma-separated file paths to scan (required when scope is "files")'),
  },
  execute: async (args) => {
    const cwd = process.cwd();
    const findings: SecretFinding[] = [];

    try {
      let filesToScan: Array<{ path: string; content: string }> = [];

      if (args.scope === 'diff') {
        // Scan only changed lines from git diff
        const diffOutput = execSync(
          'git diff HEAD -U0 2>/dev/null || git diff --cached -U0 2>/dev/null || echo ""',
          { cwd, encoding: 'utf-8', timeout: 15000, maxBuffer: 5 * 1024 * 1024 }
        );

        let currentFile = '';
        const diffLines: Array<{ file: string; line: number; content: string }> = [];

        for (const line of diffOutput.split('\n')) {
          if (line.startsWith('+++ b/')) {
            currentFile = line.substring(6);
          } else if (line.startsWith('@@')) {
            // Parse hunk header for line numbers
          } else if (line.startsWith('+') && !line.startsWith('+++')) {
            diffLines.push({ file: currentFile, line: 0, content: line.substring(1) });
          }
        }

        // Scan diff lines for patterns and entropy
        for (const dl of diffLines) {
          if (SKIP_PATTERNS.some(p => p.test(dl.file))) continue;

          // Pattern matching
          for (const sp of SECRET_PATTERNS) {
            if (sp.pattern.test(dl.content)) {
              findings.push({
                file: dl.file,
                line: dl.line,
                type: sp.name,
                severity: sp.severity,
                detail: `Pattern match: ${sp.name}`,
                detectionMethod: 'pattern',
              });
            }
          }

          // Entropy analysis
          for (const hs of extractHighEntropyStrings(dl.content)) {
            findings.push({
              file: dl.file,
              line: dl.line,
              type: 'High-entropy string',
              severity: 'medium',
              detail: `Entropy ${hs.entropy.toFixed(2)}: "${hs.value}"`,
              detectionMethod: 'entropy',
            });
          }
        }
      } else {
        // Full file scanning
        let paths: string[];

        if (args.scope === 'files' && args.paths) {
          paths = args.paths.split(',').map(p => p.trim());
        } else {
          // Scan all tracked files
          const gitFiles = execSync(
            'git ls-files 2>/dev/null || find . -type f -not -path "*/node_modules/*" -not -path "*/.git/*" | head -500',
            { cwd, encoding: 'utf-8', timeout: 15000, maxBuffer: 2 * 1024 * 1024 }
          ).trim();
          paths = gitFiles.split('\n').filter(Boolean);
        }

        for (const filePath of paths) {
          if (SKIP_PATTERNS.some(p => p.test(filePath))) continue;

          try {
            const fullPath = path.join(cwd, filePath);
            if (!fs.existsSync(fullPath)) continue;

            const stat = fs.statSync(fullPath);
            // Skip large files (likely binary or generated)
            if (stat.size > 512 * 1024) continue;

            const content = fs.readFileSync(fullPath, 'utf-8');
            const lines = content.split('\n');

            for (let i = 0; i < lines.length; i++) {
              const line = lines[i];

              // Pattern matching
              for (const sp of SECRET_PATTERNS) {
                if (sp.pattern.test(line)) {
                  findings.push({
                    file: filePath,
                    line: i + 1,
                    type: sp.name,
                    severity: sp.severity,
                    detail: `Pattern match: ${sp.name}`,
                    detectionMethod: 'pattern',
                  });
                }
              }

              // Entropy analysis
              for (const hs of extractHighEntropyStrings(line)) {
                findings.push({
                  file: filePath,
                  line: i + 1,
                  type: 'High-entropy string',
                  severity: 'medium',
                  detail: `Entropy ${hs.entropy.toFixed(2)}: "${hs.value}"`,
                  detectionMethod: 'entropy',
                });
              }
            }
          } catch {
            // Skip unreadable files (binary, encoding issues)
          }
        }
      }

      // Deduplicate findings (same file+line+type)
      const deduped = new Map<string, SecretFinding>();
      for (const f of findings) {
        const key = `${f.file}:${f.line}:${f.type}`;
        if (!deduped.has(key)) deduped.set(key, f);
      }

      const finalFindings = Array.from(deduped.values());

      return JSON.stringify({
        scope: args.scope,
        findings: finalFindings,
        criticalCount: finalFindings.filter(f => f.severity === 'critical').length,
        highCount: finalFindings.filter(f => f.severity === 'high').length,
        mediumCount: finalFindings.filter(f => f.severity === 'medium').length,
        totalCount: finalFindings.length,
        clean: finalFindings.length === 0,
      }, null, 2);
    } catch (e) {
      return JSON.stringify({
        scope: args.scope,
        error: (e as Error).message,
        findings: [],
        totalCount: 0,
        clean: false,
      }, null, 2);
    }
  },
});
```

---

## 4. Tool Registration

#### `src/tools/index.ts`

```typescript
export { detect_domains } from './domain-detector';
export { extract_code_blocks } from './file-extractor';
export { gitingest, fetchGitingest, type GitingestArgs } from './gitingest';
// v6.0
export { diff } from './diff';
// v6.3
export { imports } from './imports';
export { lint } from './lint';
export { secretscan } from './secretscan';
```

#### `src/index.ts`

Update tool imports and registration:

```typescript
import { detect_domains, extract_code_blocks, gitingest, diff, imports, lint, secretscan } from './tools';

// In the return object:
tool: {
  detect_domains,
  extract_code_blocks,
  gitingest,
  diff,
  imports,
  lint,
  secretscan,
},
```

---

## 5. Architect Workflow — Complete Pre-Reviewer Pipeline

The Phase 5 execute sequence is updated to include the full pre-reviewer pipeline. This replaces the Phase 5 from v6.2.

#### `src/agents/architect.ts` — Phase 5 (final for v6.3)

```
### Phase 5: Execute
For each task (respecting dependencies):

5a. **UI DESIGN GATE** (Rule 9): If UI task → {{AGENT_PREFIX}}designer scaffold first
5a1. {{AGENT_PREFIX}}coder - Implement
--- PRE-REVIEWER PIPELINE (tools, not agents) ---
5b. Run `diff` tool → if `hasContractChanges`:
    - Run `imports` tool for each changed file → get consumer list
    - If consumers would break → send affected files + lines to {{AGENT_PREFIX}}coder. Retry from 5a1.
    - If compatible or no contract changes → proceed
5c. Run `lint` tool (mode: "fix") → auto-apply fixable issues
5c2. Run `lint` tool (mode: "check") → if errors remain → send lint errors to {{AGENT_PREFIX}}coder. Retry from 5a1.
5c3. Run `secretscan` tool (scope: "diff") → if findings with severity critical or high → send findings to {{AGENT_PREFIX}}coder. Retry from 5a1.
--- REVIEWER PIPELINE (agents) ---
5d. {{AGENT_PREFIX}}reviewer - General review (specify CHECK dimensions)
5e. **REVIEW GATE**: APPROVED → 5e2. REJECTED → coder retry from 5a1.
5e2. **SECURITY GATE** (Rule 7): If match → security-only review. REJECTED → coder retry from 5a1.
--- TEST PIPELINE (agents) ---
5f. {{AGENT_PREFIX}}test_engineer - Verification tests + COVERAGE. If <70% → additional test pass.
5g. If FAIL → coder fixes → retry from 5d.
5h. {{AGENT_PREFIX}}test_engineer - Adversarial tests.
5i. If FAIL → coder fixes → retry from 5d.
5j. Update plan.md [x], next task.
```

Add to `## AGENTS` section, under `Available Tools`:

```
Available Tools:
  diff — Structured git diff with contract change detection
  imports — AST-based import graph: who imports a file, which exports, which lines
  lint — Run project linter. Modes: "fix" (auto-apply) or "check" (report only). Structured diagnostics.
  secretscan — Entropy-based secret detection. Scans diff or files for API keys, tokens, credentials.
```

---

## 6. Configuration Schema

#### `src/config/schema.ts`

Add to `PluginConfigSchema`:

```typescript
lint: z.object({
  enabled: z.boolean().default(true),
  auto_fix: z.boolean().default(true),
  fail_on_warnings: z.boolean().default(false),
}).optional(),

secretscan: z.object({
  enabled: z.boolean().default(true),
  entropy_threshold: z.number().min(3.0).max(6.0).default(4.5),
  skip_test_files: z.boolean().default(true),
  min_severity: z.enum(['critical', 'high', 'medium']).default('high'),
}).optional(),
```

When `lint.enabled` or `secretscan.enabled` is false, the system-enhancer injects hints to skip those pipeline steps:

#### `src/hooks/system-enhancer.ts`

```typescript
if (config.lint?.enabled === false) {
  tryInject('[SWARM CONFIG] Lint tool is DISABLED. Skip steps 5c/5c2 in pre-reviewer pipeline.');
}
if (config.secretscan?.enabled === false) {
  tryInject('[SWARM CONFIG] Secretscan tool is DISABLED. Skip step 5c3 in pre-reviewer pipeline.');
}
```

---

## 7. Test Requirements

### `imports` Tool

- Unit: Detect TS ecosystem from tsconfig.json presence.
- Unit: Detect Python ecosystem from pyproject.toml presence.
- Unit: `parseImportLine` correctly parses named imports `import { X, Y } from './mod'`.
- Unit: `parseImportLine` correctly parses default imports `import X from './mod'`.
- Unit: `parseImportLine` correctly parses namespace imports `import * as X from './mod'`.
- Unit: `parseImportLine` correctly parses require `const { X } = require('./mod')`.
- Unit: `parseImportLine` correctly parses re-exports `export { X } from './mod'`.
- Unit: `parseImportLine` correctly parses dynamic imports `import('./mod')`.
- Unit: `isImportOfTarget` resolves relative paths correctly.
- Unit: `isImportOfTarget` rejects bare specifiers (node_modules).
- Unit: Self-references (file importing itself) are excluded.
- Unit: Symbol filter works — only returns consumers of the specified symbol.
- Integration: Given a TS project with file A importing from file B, `imports(file: B)` returns A.
- Integration: Given a Python project with module A importing from module B, `imports(file: B)` returns A.

### `lint` Tool

- Unit: Detect Biome from biome.json.
- Unit: Detect ESLint from .eslintrc.js.
- Unit: Detect Ruff from pyproject.toml with `[tool.ruff]`.
- Unit: Detect clippy from Cargo.toml.
- Unit: Detect PSScriptAnalyzer from .ps1 file presence.
- Unit: Fallback to package.json lint script.
- Unit: Return "no linter detected" when nothing found.
- Unit: `parseESLintOutput` normalizes severity correctly.
- Unit: `parseRuffOutput` normalizes fixable flag correctly.
- Unit: `parseGenericOutput` extracts file:line:col:message pattern.
- Integration: Fix mode applies auto-fixes and reports remaining.
- Integration: Check mode reports without modifying files.
- Integration: Linter exit code non-zero (findings present) does not crash tool.

### `secretscan` Tool

- Unit: `shannonEntropy` returns ~1.0 for "aaab", ~3.32 for "abcdefghij".
- Unit: `shannonEntropy` returns 0 for empty string.
- Unit: `extractHighEntropyStrings` finds high-entropy quoted strings.
- Unit: `extractHighEntropyStrings` ignores short strings (<20 chars).
- Unit: `extractHighEntropyStrings` ignores low-entropy strings (repeated chars).
- Unit: Pattern match for AWS access key `AKIAIOSFODNN7EXAMPLE`.
- Unit: Pattern match for GitHub token `ghp_xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx`.
- Unit: Pattern match for OpenAI key `sk-xxxxxxxxxxxxxxxxxxxxxxxx`.
- Unit: Pattern match for private key header `-----BEGIN RSA PRIVATE KEY-----`.
- Unit: Pattern match for connection string `postgres://user:pass@host/db`.
- Unit: Skip patterns exclude node_modules, .git, test files.
- Unit: Diff scope parses git diff output correctly.
- Unit: Deduplication removes same file+line+type findings.
- Integration: Diff scope scans only added lines (not removed).
- Integration: All scope scans tracked files, skips large/binary files.
- Integration: Clean project returns `{ clean: true, totalCount: 0 }`.

---

## 8. Migration Notes

- **Three new tools are additive.** No existing tools or agents are removed or modified.
- **Rule 8 is updated** to prefer `imports` over explorer for integration analysis. Explorer is retained as fallback for unsupported ecosystems.
- **Phase 5 workflow changes** affect users with custom architect prompts. Users with `customAppendPrompt` get changes automatically.
- **Config keys are optional** with sensible defaults. Existing configs work without modification.
- **The pre-reviewer pipeline runs sequentially**: diff → imports → lint fix → lint check → secretscan. Each step short-circuits back to coder if issues are found, so the reviewer only sees clean code.

---

## 9. Implementation Order

1. `src/tools/imports.ts` — New file. No external dependencies beyond `node:child_process`.
2. `src/tools/lint.ts` — New file. Relies on project's own linter being installed.
3. `src/tools/secretscan.ts` — New file. Zero external dependencies.
4. `src/tools/index.ts` — Add exports for all three.
5. `src/index.ts` — Register all three tools.
6. `src/config/schema.ts` — Add `lint` and `secretscan` config blocks.
7. `src/agents/architect.ts` — Update Rule 8, add tools to `Available Tools`, rewrite Phase 5 with full pre-reviewer pipeline.
8. `src/hooks/system-enhancer.ts` — Add config-disabled hints.
9. Tests for all of the above.
